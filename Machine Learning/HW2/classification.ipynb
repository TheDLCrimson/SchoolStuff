{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "In this question, we will work on a [Kaggle competition](https://www.kaggle.com/t/24eae58e93964645985d075969ab1eb8). First, you need to download the dataset from the competition.\n",
    "\n",
    "We will train a model for binary sentiment classification task. The dataset consists of reviews for businesses, including restaurants, bars, dentists, ..., and the output is positive or negative. \n",
    "\n",
    "First, we need to load the training dataset from csv file. It has two columns, each line is text input and the corresponding binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data['text']\n",
    "y_train = [1  if _ == 'TRUE' else 0 for _ in data['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a simple preprocessing step that removes punctuations and converts text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuation(x):\n",
    "    return re.sub(r'[^\\w\\s]', '', x)\n",
    "def prepropess(x):\n",
    "    x = remove_punctuation(x)\n",
    "    x = x.lower()\n",
    "    return x\n",
    "\n",
    "x_train = [prepropess(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since the model can not read text directly, we need to transform them such that the model can understand. For that purpose, we will use tf-idf (term frequency–inverse document frequency) to encode the input to a vector in which each value corresponds to a word.\n",
    "\n",
    "#### Count vectorization\n",
    "A document includes many words, which may have different contributions to the label. Intuitively, a word that is mentioned more in the text should be more important and should be assigned higher weight. The simplest way is assigning weights based on the number of occurences of words in the document. This is referred to as term frequency (tf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_count_dict(data):\n",
    "    word_dict = defaultdict(int)\n",
    "    for word in data.split():\n",
    "        word_dict[word] += 1\n",
    "    return word_dict\n",
    "\n",
    "def get_word_set(data):\n",
    "    word_set = set()\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            word_set.add(word)\n",
    "    return word_set\n",
    "\n",
    "word_set = get_word_set(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(doc):\n",
    "    word_dict = get_count_dict(doc)\n",
    "    doc = doc.split()\n",
    "    num_words = len(doc)\n",
    "    for word in word_dict:\n",
    "        word_dict[word] = word_dict[word] / float(num_words)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data = []\n",
    "for doc in x_train:\n",
    "    tf_data.append(compute_tf(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use ```CountVectorizer``` from ```sklean``` package, however it does not normalize by the number of words in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'zztaine' 'zzzzaaaacccchhh' 'émigré']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_x_train = vectorizer.fit_transform(x_train)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "vectorized_x_train[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams\n",
    "In above example, we consider each word separately. Nevertheless, in text data, some words usually occur together, for instance \"machine learning\", \"linear regression\", ..., and this information is ignored in this vectorization method.\n",
    "\n",
    "To include more meaningful information to text features, we can consider a group of n words, which is called n-gram, as a feature and count the occurrence. To avoid high number of features, we only use small n (1-gram, 2-grams, 3-grams). This is done by simply setting the ```ngram_range``` in ```CountVectorizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '00 more' '00 more positive' ... 'émigré' 'émigré who'\n",
      " 'émigré who in']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "vectorized_x_train = vectorizer.fit_transform(x_train)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "vectorized_x_train[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "Term frequency itself has a serious problem: it only considers the importance of words in that document. When all documents are about the same topic, for instance *movie reviews*, \"movie\", \"actor\", ... will appear many times but convey no information about the semantic meaning of the review. We therefore apply inverse document frequency (idf) to downweight words by the number of documents that contains the word. We compute idf as follows\n",
    "$$\n",
    "\\operatorname{idf}(w)=\\log\\frac{N}{\\operatorname{df}(w)}\n",
    "$$\n",
    "where $N$ is the total number of documents in the datasets, $\\operatorname{df}(w)$ is the number of documents that contains word $w$.\n",
    "\n",
    "Finally, we combine two definition together and compute tf-idf of a word $w$ in the document $d$ by\n",
    "$$\n",
    "\\operatorname{tf-idf}(w,d)=\\operatorname{tf}(w,d) .\\operatorname{idf}(w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_idf(data):\n",
    "    import math\n",
    "    N = len(data)\n",
    "    idf = defaultdict(int)\n",
    "    for doc in data:\n",
    "        word_dict = get_count_dict(doc)\n",
    "        for k, v in word_dict.items():\n",
    "            idf[k] += 1\n",
    "    for word, _ in idf.items():\n",
    "        idf[word] = math.log(N / float(idf[word]))\n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tfidf(doc, idf):\n",
    "    tf = compute_tf(doc)\n",
    "    tfidf = defaultdict(int)\n",
    "    for word, value in tf.items():\n",
    "        tfidf[word] = value * idf[word]\n",
    "    return tfidf\n",
    "\n",
    "idf = compute_idf(x_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = []\n",
    "for doc in x_train:\n",
    "    tfidf_data.append(compute_tfidf(doc, idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You obtain a list ```tfidf_data``` of dictionary corresponding to each document where keys are words and values are their tfidf in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can also use ```TfidfVectorizer``` from ```sklearn``` to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0000' ... 'zztaine' 'zzzzaaaacccchhh' 'émigré']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_x_train = vectorizer.fit_transform(x_train)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "vectorized_x_train[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tasks is using classification models that we learnt so far, including kNN, Decision Trees, Perceptron, to predict sentiment of each review in ```test_data.csv``` and ```priv_test_data.csv``` file. \n",
    "- You need to try every vectorization method we have learnt in this notebook, including count vectorization, n-grams, tf-idf.\n",
    "- You will submit an output ```.csv``` file to Kaggle. Details on how to submit can be found here [competition](https://www.kaggle.com/t/24eae58e93964645985d075969ab1eb8). You can try any technique to improve the result, e.g. change objective function, add regularization, preprocess the data, ... You are allowed to use any external package for preprocessing step or learning the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(text):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    vectorized = vectorizer.fit_transform(text)\n",
    "    return vectorized\n",
    "\n",
    "def tfidf(text):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer = vectorizer.fit(text)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(x_train, y_train, test_size=0.22, random_state=1)\n",
    "y_train = [1  if _ == 'TRUE' else 0 for _ in data['label']]\n",
    "\n",
    "def train(X_train, y_train, vecterizer, model):\n",
    "    vectorizer = vecterizer(X_train)\n",
    "    vectorized_x_train = vectorizer.transform(X_train)\n",
    "    model = model.fit(vectorized_x_train, y_train)\n",
    "    return model, vectorizer\n",
    "\n",
    "model, vectorizer = train(x_train, y_train, tfidf, DecisionTreeClassifier(max_depth=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predict(file, vectorizer, model):\n",
    "    data = pd.read_csv(file)\n",
    "    x = data['text']\n",
    "    x = [prepropess(text) for text in x]\n",
    "    vectorized_x = vectorizer.transform(x)\n",
    "    labels = model.predict(vectorized_x)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = load_predict('data/test_data.csv', vectorizer, model)\n",
    "y_pri_test = load_predict('data/priv_test_data.csv', vectorizer, model)\n",
    "\n",
    "y_all = np.concatenate((y_test, y_pri_test))\n",
    "\n",
    "result = pd.DataFrame({'id':range(len(y_all)), 'label': y_all})\n",
    "result.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
